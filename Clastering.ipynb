{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as spatial\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(stopWords, text):\n",
    "    return [w for w in text if w not in stopWords and len(w)>2]\n",
    "\n",
    "class Tweet:\n",
    "    def __init__(self, id, text, time, stopWords, bounding_box = None, coordinates = None):\n",
    "        self.id = id\n",
    "        self.text = RemoveStopWords(stopWords, \n",
    "                                    list(filter(None,re.split('[^a-z]',text.lower()))))\n",
    "        self.time = parser.parse(str(time))\n",
    "        if bounding_box == None:\n",
    "            self.bounding_box = []\n",
    "        else:\n",
    "            self.bounding_box = bounding_box\n",
    "        self.coordinates = coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9721it [00:08, 1158.37it/s]\n",
      "862it [00:00, 1486.39it/s]\n",
      "100%|██████████| 7865/7865 [00:00<00:00, 1194921.61it/s]\n",
      " 16%|█▌        | 1227/7865 [00:00<00:00, 12266.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Number of nonloc is 7322\n",
      "and number of bbox is 543\n",
      "Dictionary is generated. Number of words 17375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7865/7865 [00:00<00:00, 10859.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is calculated. Shape is (7865, 17375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "nonloc_values = [Tweet(json.loads(line)[\"_id\"][\"$numberLong\"],\n",
    "                       json.loads(line)[\"text\"], \n",
    "                       json.loads(line)[\"created_at\"],\n",
    "                       stopWords)\n",
    "                for line \n",
    "                in tqdm(open(\"actual_data/nogeo.json\"))\n",
    "                if json.loads(line)[\"lang\"] == \"en\"\n",
    "                    and json.loads(line)[\"in_reply_to_user_id\"] == None \n",
    "                    and json.loads(line)[\"in_reply_to_status_id\"] == None\n",
    "                    and json.loads(line)[\"retweeted\"] == False]\n",
    "#nonloc_values = RemoveStopWords(stopWords, nonloc_values)\n",
    "\n",
    "bbox_values = [Tweet(json.loads(line)[\"_id\"][\"$numberLong\"],\n",
    "                     json.loads(line)[\"text\"], \n",
    "                     json.loads(line)[\"created_at\"],\n",
    "                     stopWords,\n",
    "                bounding_box = json.loads(line)[\"place\"][\"bounding_box\"][\"coordinates\"][0])\n",
    "                for line \n",
    "                in tqdm(open(\"actual_data/bbox.json\"))\n",
    "                if json.loads(line)[\"lang\"] == \"en\"\n",
    "                    and json.loads(line)[\"in_reply_to_user_id\"] == None \n",
    "                    and json.loads(line)[\"in_reply_to_status_id\"] == None\n",
    "                    and json.loads(line)[\"retweeted\"] == False]\n",
    "#bbox_values = RemoveStopWords(stopWords, bbox_values)\n",
    "\n",
    "print(\"Data loaded. Number of nonloc is %d\"%len(nonloc_values))\n",
    "print(\"and number of bbox is %d\"%len(bbox_values))\n",
    "\n",
    "tweet_content = [value.text for value in tqdm(bbox_values + nonloc_values)]\n",
    "#flatten the list of lists to 1d array\n",
    "flatten_content = [item for sublist in tweet_content for item in sublist]\n",
    "#remove duplicates\n",
    "content_dict = {w:'' for w in flatten_content}\n",
    "#enumerate without duplicates\n",
    "content_enum = {w: idx for idx, w in enumerate(content_dict)}\n",
    "\n",
    "print(\"Dictionary is generated. Number of words %d\"%len(content_enum))\n",
    "\n",
    "conjunction_matrix = np.zeros((len(bbox_values + nonloc_values), len(content_enum)), dtype=int) \n",
    "d = dict()\n",
    "for idx, tweet in enumerate(tqdm(bbox_values + nonloc_values)):\n",
    "    d[tweet.id] = idx\n",
    "    for w in tweet.text:\n",
    "        conjunction_matrix[idx,content_enum[w]] += 1.\n",
    "        \n",
    "print(\"Matrix is calculated. Shape is\", conjunction_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7322 [00:00<?, ?it/s]/Users/alexander/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 7322/7322 [02:18<00:00, 52.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could recognize 55.544933 per cent tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from datetime import timedelta\n",
    "new_col = []\n",
    "from lshash import lshash\n",
    "\n",
    "lsh = lshash.LSHash(6, len(content_enum))\n",
    "\n",
    "bbox_dict = {}\n",
    "\n",
    "for bbox in bbox_values: \n",
    "    lsh.index(conjunction_matrix[d[bbox.id]], extra_data=bbox.id)\n",
    "    bbox_dict[bbox.id] = bbox\n",
    "    \n",
    "#make a threshold for similarity\n",
    "threshold = 0.9\n",
    "\n",
    "for tweet in tqdm(nonloc_values):\n",
    "    cs = lsh.query(conjunction_matrix[d[tweet.id]], num_results=10, distance_func='cosine')\n",
    "    points = []\n",
    "    boxes = []\n",
    "    cs2 = []\n",
    "    for m in cs:\n",
    "        if m[1]<threshold:\n",
    "            cs2.append([m[0][1], m[1]])\n",
    "    for idx in cs2:\n",
    "        tdelta = (bbox.time-tweet.time)/timedelta(minutes=1)\n",
    "        #another threshold by time. Not more than a week\n",
    "        if tdelta > 60*24*7: continue\n",
    "        bbox = bbox_dict[idx[0]]\n",
    "        points+=[(x, tdelta+0.0001, idx[1]+0.0001)\n",
    "                 for x in bbox.bounding_box]\n",
    "        boxes.append(Polygon(bbox.bounding_box))\n",
    "    x0 = np.sum([x[0][0]*(1/x[1]+1/x[2]) for x in points])\n",
    "    y0 = np.sum([x[0][1]*(1/x[1]+1/x[2]) for x in points])\n",
    "    m0 = np.sum([1/x[1]+1/x[2] for x in points])\n",
    "    coord_res = Point([x0/m0, y0/m0])\n",
    "    for box in boxes:\n",
    "        if box.contains(coord_res):\n",
    "            tweet.bounding_box = box.exterior.coords\n",
    "            new_col.append(tweet)\n",
    "            break\n",
    "\n",
    "print(\"We could recognize %f per cent tweets\"%(len(new_col)*100.0/len(nonloc_values)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexander/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:67: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "753\n"
     ]
    }
   ],
   "source": [
    "exact_values = [Tweet(json.loads(line)[\"_id\"][\"$numberLong\"].encode(\"utf-8\"),\n",
    "                filter(None,re.split('[^a-z]', \n",
    "                    json.loads(line)[\"text\"].lower().encode(\"utf-8\"))),\n",
    "                coordinates = json.loads(line)[\"coordinates\"][\"coordinates\"])\n",
    "                for line \n",
    "                in open(\"actual_data/exact.json\")]\n",
    "exact_values = RemoveStopWords(stopWords, exact_values)\n",
    "\n",
    "exact = [value.text for value in exact_values + bbox_values]\n",
    "#flatten the list of lists to 1d array\n",
    "exact_flatten = [item for sublist in exact for item in sublist]\n",
    "#remove duplicates\n",
    "exact_dict = {w:'' for w in exact_flatten}\n",
    "#enumerate without duplicates\n",
    "exact_enum = {w: idx for idx, w in enumerate(exact_dict)}\n",
    "\n",
    "exact_matrix = np.zeros((len(exact_values+bbox_values), len(exact_enum)), dtype=int) \n",
    "d = dict()\n",
    "for idx, tweet in enumerate(exact_values + bbox_values):\n",
    "    d[tweet.id] = idx\n",
    "    for w in tweet.text:\n",
    "        exact_matrix[idx,exact_enum[w]] += 1\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "from lshash import LSHash\n",
    "i=0\n",
    "for tweet in bbox_values:\n",
    "    inside_tweets = [value for value in exact_values \n",
    "                     if Polygon(tweet.bounding_box).contains(\n",
    "                     Point(value.coordinates))]\n",
    "    lsh = LSHash(12, len(exact_enum))\n",
    "    if len(inside_tweets)<3: continue\n",
    "    for insider in inside_tweets: \n",
    "        lsh.index(exact_matrix[d[insider.id]])\n",
    "    \n",
    "    cs = lsh.query(exact_matrix[d[tweet.id]], num_results=3)\n",
    "    points = []\n",
    "    for insider in inside_tweets:\n",
    "        if exact_matrix[d[insider.id]] in cs:\n",
    "            points.append(insider.coordinates)\n",
    "    if len(cs)!=0:\n",
    "        i+=1\n",
    "print i\n",
    "print len(bbox_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
